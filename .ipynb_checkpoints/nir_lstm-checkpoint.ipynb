{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a61516-06f1-49fe-8107-f101728ad356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "    pipeline, TrainingArguments, Trainer, DataCollatorForTokenClassification)\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4644c8-d4b5-406b-848a-7962efd5adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ner.csv\")\n",
    "data.dropna(inplace=True)\n",
    "del data['POS']\n",
    "del data['Sentence #']\n",
    "data = data.rename(columns={'Sentence': 'tokens','Tag': 'ner_tags'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1afb3ad-57b3-4d49-89d3-18296667c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "import ast\n",
    "\n",
    "for i in range(len(data)):\n",
    "    tags = ast.literal_eval(data['ner_tags'][i])\n",
    "    data['ner_tags'][i] = [str(word.upper()) for word in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c41525e5-a411-4682-8eba-f98fa5b574f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ART',\n",
       " 'B-EVE',\n",
       " 'B-GEO',\n",
       " 'B-GPE',\n",
       " 'B-NAT',\n",
       " 'B-ORG',\n",
       " 'B-PER',\n",
       " 'B-TIM',\n",
       " 'I-ART',\n",
       " 'I-EVE',\n",
       " 'I-GEO',\n",
       " 'I-GPE',\n",
       " 'I-NAT',\n",
       " 'I-ORG',\n",
       " 'I-PER',\n",
       " 'I-TIM',\n",
       " 'O'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_codes = set([val for sublist in data['ner_tags'].values for val in sublist])\n",
    "entity_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9caa91e-5ea2-4507-bc1f-bea21ae3fb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-PER': 0, 'I-GPE': 1, 'I-EVE': 2, 'B-EVE': 3, 'B-ART': 4, 'B-GEO': 5, 'I-GEO': 6, 'I-ART': 7, 'B-GPE': 8, 'B-NAT': 9, 'O': 10, 'I-PER': 11, 'I-NAT': 12, 'B-TIM': 13, 'B-ORG': 14, 'I-TIM': 15, 'I-ORG': 16, 'UNK': 17} \n",
      " {0: 'B-PER', 1: 'I-GPE', 2: 'I-EVE', 3: 'B-EVE', 4: 'B-ART', 5: 'B-GEO', 6: 'I-GEO', 7: 'I-ART', 8: 'B-GPE', 9: 'B-NAT', 10: 'O', 11: 'I-PER', 12: 'I-NAT', 13: 'B-TIM', 14: 'B-ORG', 15: 'I-TIM', 16: 'I-ORG', 17: 'UNK'}\n"
     ]
    }
   ],
   "source": [
    "label2id = {}\n",
    "dict([ (elem, 0) for elem in entity_codes ])\n",
    "k=0\n",
    "for i in entity_codes:\n",
    "    label2id[i]=k\n",
    "    k+=1\n",
    "label2id['UNK']=k \n",
    "id2label = {y: x for x, y in label2id.items()}\n",
    "print(label2id,\"\\n\",id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410bac72-a277-4e45-a3b6-37b5578e3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "data['labels'] = data['ner_tags']\n",
    "for i in range(len(data)):\n",
    "    data['tokens'][i] = data['tokens'][i].lower().split()\n",
    "    data['labels'][i] = [label2id[x] for x in data['labels'][i]]\n",
    "    if len(data['ner_tags'][i]) != len(data['tokens'][i]):\n",
    "        data.drop([i],inplace=True)\n",
    "data['labels'] = data['labels'].apply(lambda x: [int(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a007e28-206e-4eea-bf24-6ad24e3400d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090ac7ef-ea32-4808-bce1-ce20071ee90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab = build_vocab_from_iterator(data['tokens'], specials=[\"<unk>\"])\n",
    "\n",
    "X = torch.nn.utils.rnn.pad_sequence([torch.tensor([vocab[y] for y in x]) for x in data_train['tokens']], batch_first=True, padding_value = vocab[\"<unk>\"]).to(\"cuda:0\")\n",
    "Y = torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in data_train['labels'].to_list()], batch_first=True, padding_value = 17).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c9afbe-cfaa-4f42-afdd-04bfaf1089ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:03<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3141.937636613846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:03<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 982.0556359291077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:03<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 692.4617780447006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:04<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 532.6275644302368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:05<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 427.3578935265541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:04<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 353.73111110925674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:04<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 302.8136973977089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:06<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 273.10426196455956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:06<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 243.9323025047779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [01:05<00:00,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 214.25425785779953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size=128\n",
    "dataset = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim = len(vocab), embedding_dim = 200, hidden_dim = 200, output_dim = len(label2id)):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim,  batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm1_out, _ = self.lstm1(embedded)\n",
    "        lstm2_out, _ = self.lstm2(lstm1_out)\n",
    "        lstm3_out, _ = self.lstm3(lstm2_out)\n",
    "        lstm4_out, _ = self.lstm4(lstm3_out)\n",
    "        tag_space = self.fc(lstm4_out)\n",
    "        return tag_space\n",
    "\n",
    "# Инициализация модели, функции потерь и оптимизатора\n",
    "model = LSTMModel()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for sentences, tags in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sentences)\n",
    "        loss = criterion(outputs.view(-1, len(label2id)), tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_size\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df4c3e68-beb6-4641-8da8-6a096d1470ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9591/9591 [00:56<00:00, 168.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predicted_labels = []\n",
    "\n",
    "for i in tqdm(data_test['tokens'].to_list()):\n",
    "    a = torch.tensor([vocab[y.lower()] for y in i]).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(a)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted_labels.append([id2label[x] for x in predicted.tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "006c9bbc-4aa7-4e32-a696-00d95d72e4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'57.56%'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=0\n",
    "for v1,v2 in zip(data_test['ner_tags'].to_list(), predicted_labels):\n",
    "    if v1==v2:\n",
    "        k+=1\n",
    "\"{:.2%}\".format(k/len(data_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6032e43c-a848-4703-b856-dc31813b0447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
